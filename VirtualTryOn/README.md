
# Virtual Try-on with image using Deep Learning

Online shopping differs greatly from conventional shopping in that you are able to shop anytime and anywhere, whereas traditional shopping may require a longer time and you can see and feel the items before you buy them. You can easily obtain both mild and high-quality goods with online shopping, while taking into account the affordability of the goods. Many products are available for purchase through online stores. It is possible to purchase almost anything from companies that offer their products. Books, clothing, household appliances, toys, hardware, software, and health insurance can all be purchased online. There are a number of online giants that sell goods over the Internet, but Flipkart.com, Amazon.com, e.Bay.com, etc. are the largest.

Online shopping has increased significantly over the last few years, according to a number of studies. In research, it has been shown that online shopping behavior is not based solely on demographic characteristics such as age, gender, and occupation.

Intriguingly, fashion has an experience-based nature, which makes it the perfect candidate for augmented reality. Consumers are, of course, the most important aspect of any business, but businesses often have difficulty becoming more aware of them. In order to address these issues holistically, Virtual Reality and Augmented Reality can be used.

This project's purpose is to provide the world with an easy way to try clothes without having to physically put them on. Research is currently being conducted on this problem statement and we will be focusing on the fit and appearance of the garments based on the customer who is purchasing them. Virtual try-on has been studied and researched a lot using various deep learning techniques. According to the survey, retailers have a huge opportunity to differentiate and drive sales through virtual try-on, especially as consumers expect the best experience from brands.

This problem can be approached in multiple ways:

- Identify the upper body of the model image to get the color-coded skeleton pose estimation
- The reference garment should be modified to fit the pose of the model image and transformed as the skeleton generated.
- Model image should be segmented for upper body in order to transfer the new style. Then stitch the transformed reference garment to the segmented model image.
- To create the new look, refine the fit and shape of the garment to get the final result of the source garment to Model.

## Code and Data Details
### Data Details:
> We use the json format for pose info as generated by OpenPose.

> Move these directories into our own dataroot data.

> You can get the processed data at [GoogleDrive](https://drive.google.com/open?id=1MxCUvKxejnwWnoZ-KoCyMCXo3TLhRuTo) or by running:

> `python data_download.py`

### Code information:
This is Virtual system to try different cloths on a 2D images to see the fit of the cloths. We have devloped three model to get the final out put of the cloths been tried on.
>Steps are named as Shape the cloth to the model, Stitch the cloth to model and Refine the cloth which is stitched to the model.
>Steps to run the models
- Execute Stage1.ipynb and generate the .pth model file
   
    parser.add_argument("stage", default="Shape", help='Shape, Stitch, Refine')
- Execute Stage2.ipynb and generate the .pth model file
   
    parser.add_argument("stage", default="Stitch", help='Shape, Stitch, Refine')
- Execute Stage3.ipynb and generate the .pth model file
    
    parser.add_argument("stage", default="Refine", help='Shape, Stitch, Refine')
    
>Once you have all the model files you will need to run them in sequence to get the final look of the models
>Execute TryonFinalStage.ipynb
    Setting to exectue this model
    
    def get_opt():
        parser = argparse.ArgumentParser()
        parser.add_argument("dataroot", default="data")
        parser.add_argument("datamode", default="train")
        parser.add_argument("stage", default="Stage1", help='Stage1, Stage2, Stage3')
        parser.add_argument('Stage1', type=str, default='pre_trained_models/Stage_1/Gan_9.pth', help='load_Stage_1_model')
        parser.add_argument('Stage2', type=str, default='pre_trained_models/Stage_2/Gan_7.pth', help='load_Stage_2_model')
        parser.add_argument('Stage3', type=str, default='pre_trained_models/Stage_3/Gan_9.pth', help='load_Stage_3_model')
        parser.add_argument('results_Stage1', type=str, default='results/test/Stage1', help='save results')
        parser.add_argument('results_Stage2', type=str, default='results/test/Stage2', help='save results')
        parser.add_argument('results_Stage3', type=str, default='results/test/Stage3', help='save results')
        parser.add_argument('model_image', default="000829_0.jpg", type=str, help='Model of the person wearing cloth')
        parser.add_argument('reference_image', default="006851_1.jpg", type=str, help='Reference cloth to swap')

        argv = ["", "data", "train", "Stage3",
                "gdrive/MyDrive/ARProject/pre_trained_models/Stage_1/Stage1.pth", 
                "gdrive/MyDrive/ARProject/pre_trained_models/Stage_2/Stage2.pth",
                "gdrive/MyDrive/ARProject/pre_trained_models/Stage_3/Stage3.pth",
                "gdrive/MyDrive/ARProject/results/test/Stage1", "results/test/Stage2/", "results/test/Stage3/",
                "000112_0.jpg", "000023_1.jpg"]
        opt = parser.parse_args(argv[1:])
        return opt
## Acknowledgements

 - Thank you [Krishna Kumar Tiwari](https://www.linkedin.com/in/agentkk/) for all the support to work on this project

## References
- Pandey, N., & Savakis, A. (2019). POLY-GAN: MULTI-CONDITIONED GAN FOR FASHION SYNTHESIS A PREPRINT. https://arxiv.org/pdf/1909.02165v1.pdf
- Pang, S., Tao, X., Xiong, N. N., & Dong, Y. (2021). An Efficient Style Virtual Try on Network for Clothing Business Industry. 1–10. http://arxiv.org/abs/2105.13183
- Andriluka, M., Pishchulin, L., Gehler, P., & Schiele, B. (2014). 2D human pose estimation: New benchmark and state of the art analysis. Proceedings of the IEEE Computer          Society Conference on Computer Vision and Pattern Recognition, 3686–3693. https://doi.org/10.1109/CVPR.2014.471
- Dong, H., Liang, X., Shen, X., Wang, B., Lai, H., Zhu, J., Hu, Z., & Yin, J. (2019). Towards multi-pose guided virtual try-on network. Proceedings of the IEEE International      Conference on Computer Vision, 2019-Octob(Iccv), 9025–9034. https://doi.org/10.1109/ICCV.2019.00912
- Hauswiesner, S., Member, S., Straka, M., & Reitmayr, G. (2013). Virtual Try-On through Image-Based Rendering. 19(9), 1552–1565.

##Code References
- https://github.com/shionhonda/viton-gan
- https://github.com/nile649/POLY-GAN
- https://github.com/minar09/cp-vton-plus
 
## Authors

- [Kiran Muloor](https://www.linkedin.com/in/kmuloor/) and [Krishna Kumar Tiwari](https://www.linkedin.com/in/agentkk/)

  
